{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNDX5EHvTs08XW4/aBj2JIP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abunchoftigers/Prediction-of-Product-Sales/blob/main/Ensemble_Trees_Exercise_(Core).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Trees Exercise (Core)"
      ],
      "metadata": {
        "id": "-X9B2IZo8ln5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " * by: David Dyer"
      ],
      "metadata": {
        "id": "s9f589QeGz_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import modeling tools"
      ],
      "metadata": {
        "id": "LIeyCTpiFIkt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCbx4jfm8a1I"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "\n",
        "from sklearn import set_config\n",
        "set_config(transform_output='pandas')\n",
        "\n",
        "from google.colab import drive\n",
        "import warnings\n",
        "\n",
        "warnings.simplefilter('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount drive"
      ],
      "metadata": {
        "id": "vBRtb01RFMiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Y4qG3XKr-3Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuration"
      ],
      "metadata": {
        "id": "7vctk5n-FQLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# configure options\n",
        "warnings.filterwarnings('ignore')\n",
        "## Display all columns\n",
        "pd.set_option('display.max_column', None)\n",
        "\n",
        "## Display all rows\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "## SK Learn Display\n",
        "set_config(display='diagram')\n",
        "\n",
        "## Transformers output as a Pandas Dataframe\n",
        "set_config(transform_output='pandas')"
      ],
      "metadata": {
        "id": "cnHeScb7-5d0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Dataset"
      ],
      "metadata": {
        "id": "KvDCmpONFSXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "fpath = '/content/drive/MyDrive/Coding Dojo - Data Science/02 - Intro to Machine Learning/Week 2/data/Boston_Housing_from_Sklearn - Boston_Housing_from_Sklearn.csv'\n",
        "df = pd.read_csv(fpath)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "jVEp4pNj-8OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# explore data\n",
        "df.info()\n",
        "df.describe()\n",
        "\n",
        "# For this assignment we're told that the dataset is all numeric and clean"
      ],
      "metadata": {
        "id": "7ReFcA-b_KCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define vectors\n",
        "X = df.drop(columns=['PRICE'])\n",
        "y = df['PRICE']\n",
        "\n",
        "# X, y"
      ],
      "metadata": {
        "id": "p_6vbwQ8_BMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train/Test Split"
      ],
      "metadata": {
        "id": "oMEbChiyFUzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "metadata": {
        "id": "ewJqib6A_GjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Regression Evaluation Function"
      ],
      "metadata": {
        "id": "B2CC1lbvFXiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "def regression_metrics(y_true, y_pred, label='', verbose = True, output_dict=False):\n",
        "  # Get metrics\n",
        "  mae = mean_absolute_error(y_true, y_pred)\n",
        "  mse = mean_squared_error(y_true, y_pred)\n",
        "  rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "  r_squared = r2_score(y_true, y_pred)\n",
        "  if verbose == True:\n",
        "    # Print Result with Label and Header\n",
        "    header = \"-\"*60\n",
        "    print(header, f\"Regression Metrics: {label}\", header, sep='\\n')\n",
        "    print(f\"- MAE = {mae:,.3f}\")\n",
        "    print(f\"- MSE = {mse:,.3f}\")\n",
        "    print(f\"- RMSE = {rmse:,.3f}\")\n",
        "    print(f\"- R^2 = {r_squared:,.3f}\")\n",
        "  if output_dict == True:\n",
        "      metrics = {'Label':label, 'MAE':mae,\n",
        "                 'MSE':mse, 'RMSE':rmse, 'R^2':r_squared}\n",
        "      return metrics\n",
        "def evaluate_regression(reg, X_train, y_train, X_test, y_test, verbose = True,\n",
        "                        output_frame=False):\n",
        "  # Get predictions for training data\n",
        "  y_train_pred = reg.predict(X_train)\n",
        "  # Call the helper function to obtain regression metrics for training data\n",
        "  results_train = regression_metrics(y_train, y_train_pred, verbose = verbose,\n",
        "                                     output_dict=output_frame,\n",
        "                                     label='Training Data')\n",
        "  print()\n",
        "  # Get predictions for test data\n",
        "  y_test_pred = reg.predict(X_test)\n",
        "  # Call the helper function to obtain regression metrics for test data\n",
        "  results_test = regression_metrics(y_test, y_test_pred, verbose = verbose,\n",
        "                                  output_dict=output_frame,\n",
        "                                    label='Test Data' )\n",
        "  # Store results in a dataframe if ouput_frame is True\n",
        "  if output_frame:\n",
        "    results_df = pd.DataFrame([results_train,results_test])\n",
        "    # Set the label as the index\n",
        "    results_df = results_df.set_index('Label')\n",
        "    # Set index.name to none to get a cleaner looking result\n",
        "    results_df.index.name=None\n",
        "    # Return the dataframe\n",
        "    return results_df.round(3)"
      ],
      "metadata": {
        "id": "iArMQ-J9CJZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and evaluate a default Bagged Trees"
      ],
      "metadata": {
        "id": "xWZgBF0gDeJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a Default Model\n",
        "bagreg = BaggingRegressor(random_state = 42)\n",
        "# Model Pipeline with default preprocessor and default model\n",
        "bagreg_pipe = make_pipeline(bagreg)\n",
        "# Fit the model pipeline on the training data only\n",
        "bagreg_pipe.fit(X_train, y_train)\n",
        "# Call custom function for evaluation\n",
        "default_bag_result = evaluate_regression(bagreg_pipe, X_train, y_train, X_test, y_test, output_frame=True)"
      ],
      "metadata": {
        "id": "XbmYn62eIRLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain list of parameters\n",
        "bagreg_pipe.get_params()"
      ],
      "metadata": {
        "id": "PQ2_nIec_M55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters to tune\n",
        "param_grid = {'baggingregressor__n_estimators': [5, 10, 20, 30, 40, 50],\n",
        "              'baggingregressor__max_samples' : [.5, .7, .9, ],\n",
        "              'baggingregressor__max_features': [.5, .7, .9 ]}\n",
        "# Instaniate the gridsearch\n",
        "gridsearch = GridSearchCV(bagreg_pipe, param_grid, n_jobs=-1, verbose=1)\n",
        "\n",
        "gridsearch.fit(X_train, y_train)\n",
        "\n",
        "gridsearch.best_params_\n",
        "\n",
        "best_bagreg_grid = gridsearch.best_estimator_\n",
        "\n",
        "best_bagreg_result = evaluate_regression(best_bagreg_grid, X_train, y_train, X_test, y_test, output_frame=True)"
      ],
      "metadata": {
        "id": "TCbt4BYP_SGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and evaluate a default random forest"
      ],
      "metadata": {
        "id": "QxylxAUfNwXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# numeric pipeline# Instantiate default random forest model\n",
        "rf = RandomForestRegressor(random_state = 42)\n",
        "# Model Pipeline\n",
        "rf_pipe = make_pipeline(rf)\n",
        "\n",
        "# Fit the model pipeline on the training data only\n",
        "rf_pipe.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "7geEkpL1_SOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use custom function to evaluate default model\n",
        "default_rf_result = evaluate_regression(rf_pipe, X_train, y_train, X_test, y_test, output_frame=True)"
      ],
      "metadata": {
        "id": "W_Sxa3xZO-SA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use GridSearchCV to tune the Random Forest model to optimize performance on the test set"
      ],
      "metadata": {
        "id": "r_BzLmMxReKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check rf params"
      ],
      "metadata": {
        "id": "7vBqz1sYSmwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_pipe.get_params()"
      ],
      "metadata": {
        "id": "5tat-jxa_SVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define params to try"
      ],
      "metadata": {
        "id": "V2MtIPWJSj0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {'randomforestregressor__max_depth': [None,10,15,20],\n",
        "          'randomforestregressor__n_estimators':[10,100,150,200],\n",
        "          'randomforestregressor__min_samples_leaf':[2,3,4],\n",
        "          'randomforestregressor__max_features':['sqrt','log2',None],\n",
        "          'randomforestregressor__oob_score':[True,False],\n",
        "          }"
      ],
      "metadata": {
        "id": "4VcIKfRvShr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the gridsearch\n",
        "gridsearch = GridSearchCV(rf_pipe, params, n_jobs=-1, cv = 3, verbose=1)\n",
        "# Fit the gridsearch on training data\n",
        "gridsearch.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "-CNfpT7rS6Q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the best params"
      ],
      "metadata": {
        "id": "p9JSd-tTTAn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gridsearch.best_params_"
      ],
      "metadata": {
        "id": "pwZxVl-aTEFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the best model"
      ],
      "metadata": {
        "id": "wDfZqy_bTSKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_rf = gridsearch.best_estimator_\n",
        "best_rf_result = evaluate_regression(best_rf, X_train, y_train, X_test, y_test, output_frame=True)"
      ],
      "metadata": {
        "id": "Ns1T8JtbTVKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions"
      ],
      "metadata": {
        "id": "t7oOu-8pXI2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Which model and model params provided the best results?"
      ],
      "metadata": {
        "id": "GUkhgfvcXKX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'default bagreg metrics:\\n {default_bag_result}\\n\\nbest bagreg metrics:\\n{best_bagreg_result}\\n\\n default rf metrics:\\n{default_rf_result}\\n\\nbest rf metrics:\\n{best_rf_result}')"
      ],
      "metadata": {
        "id": "ChsVSOBGXbKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - The default RF model did the best, with the highest R^2 score for both training and test data."
      ],
      "metadata": {
        "id": "Q_3LT1rokI5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Explain how your model will perform in deployed by referring to the metrics. Ex. How close can your stakeholders expect its predictions to be to the true value\n",
        "\n",
        " - The model's predictions will match actual outcomes approximately 83.4% of the time."
      ],
      "metadata": {
        "id": "lwYz58b6TdeR"
      }
    }
  ]
}